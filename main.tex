\documentclass{beamer}

% Pacotes b√°sicos 
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{csquotes}
\usepackage{url}
\usepackage[natbib=true,style=authoryear, backend=biber, useprefix=true]{biblatex}
\addbibresource{bibliografia.bib}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

% \setbeamertemplate{footline}[frame number]
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}



% Theme
% \usetheme{default} % Choose your preferred theme here

% Title Page
\title{Introduction to quantum reservoir computing and quantum extreme learning machines}
\author{Luca Innocenti}
% \logo{\includegraphics[scale=.03]{eqai-2023-logo.jpg}}
\date{EQAI 2023}

% Content
\begin{document}

\begin{frame}
  \titlepage
  \hfill\includegraphics[height=1.5cm]{figures/logo-unipa-2020.png}
\end{frame}

\begin{frame}
\frametitle{Classical reservoir computing (RC)}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/scheme_RC.jpg}
\end{figure}

\end{frame}


\begin{frame}
  \frametitle{Classical RC}
  \framesubtitle{What is a reservoir computer?}
  
  \begin{itemize}
      \item A \textit{reservoir computer} (RC) is a fixed, untrained, nonlinear mapping from inputs to outputs, followed by a linear readout stage, where the training takes place.
      \item The ``reservoir'' is effectively a dynamical system, driven by input time-dependent signals $t\mapsto \mathbf u(t)$, $t\in\mathbb{Z}$.
      \item RCs can process time series because the reservoir has internal memory. We can describe its state as a dynamical system
      % $x_j(t)=T_j(\mathbf x(t-1),\mathbf u(t))$.
      \begin{equation}
          \mathbf x(t)=T(\mathbf x(t-1),\mathbf u(t)),
      \end{equation}
      with $T$ mapping driving signal $\mathbf u(t)$ and previous state $\mathbf x(t-1)$ to the updated internal state $\mathbf x(t)$.
      % \item The map $T$ is fixed beforehand. Training is only performed on the output signals, via a linear estimator.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Classical RC}
  \framesubtitle{Training and readout}

  \begin{itemize}
      \item The map $T$ is fixed beforehand. Training is only performed on the output signals, via a linear estimator.
      \item The read signal at each step $t$ is a linear function of the RC variables:
      \begin{equation}
          \mathbf y(t) = W\mathbf x(t).
      \end{equation}
      \item The objective is to reproduce a function 
      \begin{equation}
          \mathbf z(t)= \mathbf z(\mathbf u^{-h}(t))
      \end{equation}
      of the last $h$ time steps.
  \end{itemize}


\end{frame}

\begin{frame}
  \frametitle{Classical RC}
  \framesubtitle{Training and readout}

  \begin{itemize}
      \item To optimise we use the loss function:
      \begin{equation}
          \operatorname{MSE}_T[\mathbf z] = \frac{1}{T}\sum_{t=1}^T \|\mathbf y(t)-\mathbf z(t)\|^2,
      \end{equation}
      where $T$ is the size of the training dataset used to estimate the MSE.
      \item Some precise statements about the universality of RCs can be made studying their \textit{information processing capacity}:
      Dambre, Joni, et al. "Information processing capacity of dynamical systems." Scientific reports 2.1 (2012): 1-7.
  \end{itemize}
  
\end{frame}

\begin{frame}
\frametitle{RNNs}

\begin{figure}
    \centering
    \includegraphics[width=1.1\linewidth]{figures/RNN_unfold.png}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Classical ELMs}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/scheme_ELM.png}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Classical ELMs}
\framesubtitle{\textit{Reservoir computing} vs \textit{extreme learning machines}}
\begin{itemize}
    \item When there is no memory, we talk of \textit{extreme learning machines} (ELMs). Here we have a \textit{fixed} nonlinear function applied to the input, and a trained linear readout.
    \item More precisely, given a training dataset $\{(\mathbf x_i^{\rm tr},\mathbf y_i^{\rm tr})\}$, and a ``reservoir function'' $f$, we look for $W$ such that
    $% \begin{equation}
        W f(\mathbf x_i^{\rm tr}) \simeq \mathbf y_i^{\rm tr}
    $. % \end{equation}
    \textit{I.e.}, solve the optimisation problem
    \begin{equation}
        \operatorname*{argmin}_W \sum_i \mathcal L(W f(\mathbf x_i^{\rm tr}), \mathbf y_i^{\rm tr}).
    \end{equation}
    \item ELMs are an extremely simple approach, but training a simple linear readout can sometimes go surprisingly far.
\end{itemize}
\end{frame}


\begin{frame}
% \frametitle{Classical RC}
\frametitle{Linear regression}

\begin{itemize}
    \item The standard choice of loss function is the $L_2$ distance. So we want to minimise $\sum_i \|W \mathbf x_i^{\rm tr}-\mathbf y_i^{\rm tr}\|_2^2$. Or equivalently, minimise $\|WX^{\rm tr}-Y^{\rm tr}\|_2$, defining $X^{\rm tr}\equiv \sum_i \mathbf x_i^{\rm tr} \mathbf e_i^\dagger$ and $Y^{\rm tr}\equiv \sum_i \mathbf y_i^{\rm tr} \mathbf e_i^\dagger$.
    \item This is (relatively) easy to do. We can characterise the solutions via the \textit{pseudoinverse}: $W = Y^{\rm tr} (X^{\rm tr})^+$. Computing $(X^{\rm tr})^+$ involves reciprocals of singular values of $X^{\rm tr}$, so this can be an ill-conditioned problem, and regularisation techniques become necessary.
    \item This works both for ELMs and RCs, changing what $\mathbf x_i^{\rm tr}$ represents.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Classical ELM}
\framesubtitle{Toy examples}
As a toy example, try to extract the angle from points in a circle (for angles $\theta\in[0,3\pi/2]$ to get a well-defined problem).
\\[10pt]

\begin{columns}
    \begin{column}{0.48\textwidth}
        % \centering
        Without nonlinearity:
        \\[6pt]
        \begin{figure}
            % \centering
            \hspace{-50pt}
            \includegraphics[width=\linewidth]{figures/ELMforanglesprediction_onlylinear.pdf}
            % \caption{Caption}
            % \label{fig:my_label}
        \end{figure}
    \end{column}
    \hspace{-50pt}
    \vrule{}
    \begin{column}{0.48\textwidth}
        \centering
        Simple 3-layer neural network and linear readout stage
        \begin{figure}
            \centering
            \includegraphics[width=\linewidth]{figures/ELMforanglesprediction.pdf}
            % \caption{Caption}
            % \label{fig:my_label}
        \end{figure}
    \end{column}
\end{columns}


\end{frame}




\begin{frame}
  \frametitle{Some references on ELMs and RCs}
  \begin{itemize}\small
      \item Wang, J., Lu, S., Wang, S. H., \& Zhang, Y. D. (2022). A review on extreme learning machine. Multimedia Tools and Applications, 81, 41611-41660.
      \item Dambre, J., Verstraeten, D., Schrauwen, B., \& Massar, S. (2012). Information processing capacity of dynamical systems. Scientific Reports, 2(1), 514.
      \item Cucchi, M., Abreu, S., Ciccone, G., Brunner, D., \& Kleemann, H. (2022). Hands-on reservoir computing: a tutorial for practical implementation. Neuromorphic Computing and Engineering, 2(3). DOI 10.1088/2634-4386/ac7db7
  \end{itemize}
\end{frame}

%
\begin{frame}

\frametitle{From classical to quantum ELMs}
\begin{figure}
\includegraphics[width=0.7\linewidth]{figures/scheme_ELM.png}
\end{figure}
\begin{figure}
\includegraphics[width=0.7\linewidth]{figures/scheme_QELM.png}
\end{figure}
\end{frame}
%

\begin{frame}
\frametitle{From classical to quantum ELMs}
% \framesubtitle{General description}

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/ELMs to QELMs.png}
\end{figure}

\begin{itemize}
    \item Replace inputs with quantum states, the nonlinear function with some quantum dynamic, and the readout stage with a linear estimator applied to measurement outcomes.
    \item We can always model such a device as some channel $\Phi$ applied to input states $\rho$, and a measurement $\boldsymbol\mu$ performed at the end, resulting in probabilities:
    \begin{equation}
        p_b(\rho) = \operatorname{tr}(\mu_b \Phi(\rho)).
    \end{equation}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{QRCs: general scheme}
% \framesubtitle{General description}
\begin{itemize}
    \item If there's memory, we have a sequence of inputs, $\rho_t, t\in\mathbb{Z}$, some internal state of the reservoir $\eta_t$, and a channel $\Phi$ acting on both. A single step then reads:
    \begin{equation}
        \eta_{t+1} = \Phi(\rho_t\otimes\eta_t).
    \end{equation}
    Two steps correspond to:
    \begin{equation}
        \eta_{t+2} = \Phi(\rho_{t+1}\otimes\Phi(\rho_t\otimes\eta_t))
        \equiv \Phi^{(2)}(\rho_{t+1}\otimes\rho_t\otimes\eta_t),
    \end{equation}
    
    % \pause
    \item \textit{This assumes no measurement in between!}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{QRC: general scheme}

\begin{figure}
\includegraphics[width=\linewidth]{figures/scheme_QRC.png}
\end{figure}

\end{frame}



\begin{frame}
\frametitle{What happens \textit{quantising} QELMs/QRCs?}
% \framesubtitle{Classical vs quantum}
Quantizing the classical scheme changes things significantly:
\begin{itemize}
    \item It stops being obvious what exactly is being ``processed'' (classical or quantum info?).
    \item We now have to deal with quantum states, only accessible through measurements. Measurements unavoidably perturb the system, so what does ``processing temporal sequences'' mean in this context?
    \item There are intrinsic constraints of what the evolution of a quantum system can look like.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Information processing in QELMs/QRCs}
% \framesubtitle{Types of information processing}
We can process classical or quantum information, depending on the problem setting:
% \begin{itemize}
%     \item 
% \end{itemize}
\begin{figure}
\includegraphics[width=0.7\linewidth]{figures/QRCs processing info.png}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Achievable targets with QELMs for state estimation}

\begin{itemize}
    \item Linearity of quantum channels and measurements allow an easy characterisation of achievable target functions: applying a linear functional $W$ to measurement probabilities gives
    \begin{equation}
        \sum_j W_j p_j = \sum_j W_j \operatorname{tr}(\mu_j \Phi(\rho)).
    \end{equation}
    Defining an ``effective measurement'' $\tilde\mu_j\equiv \Phi^\dagger(\mu_j)$, the set of all achievable observables is easily characterised as
    \begin{equation}
        \operatorname{span}_{\mathbb{R}}(\{\tilde\mu_j\}).
    \end{equation}
    \item For example, can you use a QELM to estimate the concurrence of input states? No, because that doesn't correspond to measuring an observable.
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{QELMs for state estimation, implementation}

\begin{itemize}
    \item Uncharacterised physical evolutions can be exploited for state estimation and other metrological tasks.
    \item The specific details of the evolution don't matter. As long as no specific symmetries are present, one can tomographically reconstruct input states from outputs.
    \item A large variety of physical evolutions can be used as a ``reservoir''. The only real requirement is an evolution sending inputs to output states in a sufficiently larger Hilbert space (we don't really need a ``reservoir'' to do QRC).
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{QELMs for classical processing}

\begin{itemize}
    \item If the goal is processing classical information, paying attention at how the information is encoded into the input states is paramount, as it will determine the kind of features we can hope to extract.
    \item Nonlinearity is possible here, as long as the encoding $s\mapsto \rho_s$ is nonlinear (as it usually is).
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{What can you do with QRCs?}

\begin{itemize}
    \item This framework still applies to the time-dependent case. Although now the inputs are sequences of states, $\rho_1\otimes\cdots\otimes\rho_k$, evolving through some effective channel $\tilde\Phi$. Measurement is still only performed at the end though.
    \item One can also consider QELMs, using multiple copies of the same state as input, thus allowing reconstruction of nonlinear functionals.
    \item The memory capacity of the channel $\Phi$ will determine how nonlinear the targets can be.
    \item There are known advantages in performing global measurements over multiple copies of a state (\textit{e.g.} \href{https://www.science.org/doi/10.1126/science.abn7293}{\textit{Huang et al. "Quantum advantage in learning from experiments." Science (2022)}}).
\end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Conclusion}
  \begin{itemize}
      \item QELMs/QRCs are very easy to train and setup, and easy to implement experimentally.
      \item The simplicity of the model allows to have a good understanding of what is and isn't possible, especially from the state estimation perspective.
      \item They are good examples to illustrate the problems that can arise ``quantising'' a classical model.
      \item Measurements are an inescapable part of quantum mechanics, and change what's possible quite a bit. One needs to take into account intrinsic stochastic noise of measurement outcomes when evaluating performances of quantum models.
      \item Doing ``online learning'' with quantum states is a tricky business. Plenty of open questions here.
  \end{itemize}
\end{frame}

\end{document}
